{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffq6A2-ifzAA"
   },
   "source": [
    "# Интеллектуальный анализ данных – весна 2025\n",
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPcxtekTA1Sm"
   },
   "source": [
    "Правила:\n",
    "\n",
    "\n",
    "\n",
    "*   Домашнее задание оценивается в 10 баллов.\n",
    "*   Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания.\n",
    "*  Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
    "*  Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
    "*  Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.\n",
    "* Если в задании есть вопрос на рассуждение, то за отсутствие ответа на него балл за задание будет снижен вполовину."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itRtFtrOf0_b"
   },
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов.\n",
    "\n",
    "Будем предсказывать эмоциональную окраску твиттов о коронавирусе.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "tNGRVO7_g9mz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import  List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "zOy8iHJQg_Ss",
    "outputId": "6a32c325-1b9a-4895-ab22-5e985016da91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24253</th>\n",
       "      <td>33457</td>\n",
       "      <td>78409</td>\n",
       "      <td>Arkansas, USA</td>\n",
       "      <td>05-04-2020</td>\n",
       "      <td>I better not see any gyms, dieticians, persona...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16409</th>\n",
       "      <td>23779</td>\n",
       "      <td>68731</td>\n",
       "      <td>Canada</td>\n",
       "      <td>24-03-2020</td>\n",
       "      <td>Coronavirus will drive paper, market pulp, for...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>12320</td>\n",
       "      <td>57272</td>\n",
       "      <td>Lagos, Nigeria</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>4 of 5 ...how does the #Coronavirus affect tho...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30729</th>\n",
       "      <td>41598</td>\n",
       "      <td>86550</td>\n",
       "      <td>4 8 15 16 23 42</td>\n",
       "      <td>11-04-2020</td>\n",
       "      <td>So when we see stories about food insecurity i...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName         Location     TweetAt  \\\n",
       "24253     33457       78409    Arkansas, USA  05-04-2020   \n",
       "16409     23779       68731           Canada  24-03-2020   \n",
       "7007      12320       57272   Lagos, Nigeria  19-03-2020   \n",
       "30729     41598       86550  4 8 15 16 23 42  11-04-2020   \n",
       "\n",
       "                                           OriginalTweet           Sentiment  \n",
       "24253  I better not see any gyms, dieticians, persona...            Positive  \n",
       "16409  Coronavirus will drive paper, market pulp, for...            Negative  \n",
       "7007   4 of 5 ...how does the #Coronavirus affect tho...            Positive  \n",
       "30729  So when we see stories about food insecurity i...  Extremely Negative  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_coronavirus.csv', encoding='latin-1')\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2OiDog9ZBlS"
   },
   "source": [
    "Для каждого твитта указано:\n",
    "\n",
    "\n",
    "*   UserName - имя пользователя, заменено на целое число для анонимности\n",
    "*   ScreenName - отображающееся имя пользователя, заменено на целое число для анонимности\n",
    "*   Location - местоположение\n",
    "*   TweetAt - дата создания твитта\n",
    "*   OriginalTweet - текст твитта\n",
    "*   Sentiment - эмоциональная окраска твитта (целевая переменная)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZTMseDkhTC7"
   },
   "source": [
    "## Задание 1 Подготовка (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx2-odn9hdAW"
   },
   "source": [
    "Целевая переменная находится в колонке `Sentiment`.  Преобразуйте ее таким образом, чтобы она стала бинарной: 1 - если у твитта положительная или очень положительная эмоциональная окраска и 0 - если отрицательная или очень отрицательная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ZaQKQ1zEjP15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃÂT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÃÂs first confirmed COV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                     Location     TweetAt  \\\n",
       "0      3800       48752                           UK  16-03-2020   \n",
       "1      3801       48753                    Vagabonds  16-03-2020   \n",
       "2      3802       48754                          NaN  16-03-2020   \n",
       "3      3803       48755                          NaN  16-03-2020   \n",
       "4      3804       48756  ÃÂT: 36.319708,-82.363649  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet  Sentiment  \n",
       "0  advice Talk to your neighbours family to excha...          1  \n",
       "1  Coronavirus Australia: Woolworths to give elde...          1  \n",
       "2  My food stock is not the only one which is emp...          1  \n",
       "3  Me, ready to go at supermarket during the #COV...          0  \n",
       "4  As news of the regionÃÂs first confirmed COV...          1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"] = np.where(df[\"Sentiment\"].str.contains(\"Positive\"), 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGq1FxJ-kBo5"
   },
   "source": [
    "Сбалансированы ли классы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "a7gdNtxckK5V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    18046\n",
       "0    15398\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.32638811925081"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15398/18046 * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng8BCelMkWb0"
   },
   "source": [
    "**Ответ:** При разнице менее 15% можно мы можем принять классы из колонки Sentiment как сбалансированные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmSIBSsLk5Zz"
   },
   "source": [
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их строкой 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UhUVRkR5kxa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName            0\n",
       "ScreenName          0\n",
       "Location         7049\n",
       "TweetAt             0\n",
       "OriginalTweet       0\n",
       "Sentiment           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName         0\n",
       "ScreenName       0\n",
       "Location         0\n",
       "TweetAt          0\n",
       "OriginalTweet    0\n",
       "Sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Location\"] = df[\"Location\"].fillna(\"Unknown\")\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>4495</td>\n",
       "      <td>49447</td>\n",
       "      <td>Strensall, North Yorkshire, UK</td>\n",
       "      <td>17-03-2020</td>\n",
       "      <td>407 new confirmed cases in the UK Please remem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15434</th>\n",
       "      <td>22577</td>\n",
       "      <td>67529</td>\n",
       "      <td>Amelia Island, FL</td>\n",
       "      <td>23-03-2020</td>\n",
       "      <td>In response to the #Covid_19 crisis, we've low...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8095</th>\n",
       "      <td>13635</td>\n",
       "      <td>58587</td>\n",
       "      <td>London</td>\n",
       "      <td>20-03-2020</td>\n",
       "      <td>The Space Daddy is as disappointed as I am in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10214</th>\n",
       "      <td>16197</td>\n",
       "      <td>61149</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>21-03-2020</td>\n",
       "      <td>Missouri woman gives birth in the #toiletpaper...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>10886</td>\n",
       "      <td>55838</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>The outbreak of COVID 19 along with the dine i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21940</th>\n",
       "      <td>30577</td>\n",
       "      <td>75529</td>\n",
       "      <td>Waterloo, Ontario, Canada</td>\n",
       "      <td>02-04-2020</td>\n",
       "      <td>\"Meet Michael Gill, the man in charge of coron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25744</th>\n",
       "      <td>35335</td>\n",
       "      <td>80287</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>07-04-2020</td>\n",
       "      <td>Supermarket shoppers in S pore ditch baskets f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31405</th>\n",
       "      <td>42440</td>\n",
       "      <td>87392</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>11-04-2020</td>\n",
       "      <td>I love being able to learn how to cook more me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>4078</td>\n",
       "      <td>49030</td>\n",
       "      <td>On Your Timeline</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@Amazon says it needs to hire 100,000 people a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23541</th>\n",
       "      <td>32556</td>\n",
       "      <td>77508</td>\n",
       "      <td>Gold Coast</td>\n",
       "      <td>05-04-2020</td>\n",
       "      <td>@GerardDaffy I agree Gerard. I followed a coup...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                        Location     TweetAt  \\\n",
       "585        4495       49447  Strensall, North Yorkshire, UK  17-03-2020   \n",
       "15434     22577       67529               Amelia Island, FL  23-03-2020   \n",
       "8095      13635       58587                          London  20-03-2020   \n",
       "10214     16197       61149                         Unknown  21-03-2020   \n",
       "5833      10886       55838                 Toronto, Canada  19-03-2020   \n",
       "21940     30577       75529       Waterloo, Ontario, Canada  02-04-2020   \n",
       "25744     35335       80287                         Unknown  07-04-2020   \n",
       "31405     42440       87392                  United Kingdom  11-04-2020   \n",
       "231        4078       49030                On Your Timeline  16-03-2020   \n",
       "23541     32556       77508                      Gold Coast  05-04-2020   \n",
       "\n",
       "                                           OriginalTweet  Sentiment  \n",
       "585    407 new confirmed cases in the UK Please remem...          0  \n",
       "15434  In response to the #Covid_19 crisis, we've low...          0  \n",
       "8095   The Space Daddy is as disappointed as I am in ...          0  \n",
       "10214  Missouri woman gives birth in the #toiletpaper...          1  \n",
       "5833   The outbreak of COVID 19 along with the dine i...          0  \n",
       "21940  \"Meet Michael Gill, the man in charge of coron...          1  \n",
       "25744  Supermarket shoppers in S pore ditch baskets f...          0  \n",
       "31405  I love being able to learn how to cook more me...          1  \n",
       "231    @Amazon says it needs to hire 100,000 people a...          0  \n",
       "23541  @GerardDaffy I agree Gerard. I followed a coup...          0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tzt27tfjUpq"
   },
   "source": [
    "Разделите данные на обучающие и тестовые в соотношении 7 : 3 и укажите `random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "xSLOA9tIj9Z6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df[\"Sentiment\"]\n",
    "X = df.drop([\"Sentiment\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9RrPUsJlL60"
   },
   "source": [
    "## Задание 2 Токенизация (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dz_b7Xopc_R"
   },
   "source": [
    "Постройте словарь на основе обучающей выборки и посчитайте количество встреч каждого токена с использованием самой простой токенизации - деления текстов по пробельным символам и приведения токенов в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "SFr67WOJphny"
   },
   "outputs": [],
   "source": [
    "tweets = X_train[\"OriginalTweet\"]\n",
    "dictionary = dict()\n",
    "\n",
    "for tweet in tweets:\n",
    "    for word in tweet.lower().split():\n",
    "        dictionary[word] = dictionary.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0h2Jqkpnao"
   },
   "source": [
    "Какой размер словаря получился?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "umyENA7EpokD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79755"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d2G1Z-Qpqkd"
   },
   "source": [
    "Выведите 10 самых популярных токенов с количеством встреч каждого из них. Объясните, почему именно эти токены в топе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Impi32a_pssg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the             26815\n",
       "to              23373\n",
       "and             14684\n",
       "of              13012\n",
       "a               11737\n",
       "in              11198\n",
       "for              8566\n",
       "#coronavirus     8223\n",
       "is               7383\n",
       "are              7050\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_sorted = pd.Series(dictionary)\n",
    "dictionary_sorted = dictionary_sorted.sort_values(ascending=False)\n",
    "dictionary_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtuJCD0ApuFd"
   },
   "source": [
    "**Ответ:** Это предлоги, союзы, частицы и прочие слова, которые часто встречаются в предложениях и имеют только одну форму, что увеличивает частоту их появлений. Кроме того, эти слова являются стоп-словами, так как не несут смысловой нагрузки, а только связывают другие слова в предлжении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7DTQDkWsVYp"
   },
   "source": [
    "Удалите стоп-слова из словаря и выведите новый топ-10 токенов (и количество встреч) по популярности.  Что можно сказать  о нем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "8csSAdgTsnFx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#coronavirus    8223\n",
       "prices          3891\n",
       "food            3820\n",
       "grocery         3469\n",
       "supermarket     3288\n",
       "people          3175\n",
       "covid-19        3173\n",
       "store           3155\n",
       "#covid19        2471\n",
       "&amp;           2314\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "new_dictionary = {key: value for key, value in dictionary.items() if key not in stop_words}\n",
    "\n",
    "no_stop_words = pd.Series(new_dictionary)\n",
    "no_stop_words = no_stop_words.sort_values(ascending=False)\n",
    "no_stop_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZH0x2Lzs-Dh"
   },
   "source": [
    "**Ответ:**  Теперь из топа пропали служебные части речи, остались только слова имеющие смысл, что гораздо ценнее для нас при работе с текстом. Судя по словам, датасет был собран в 2020-2021 году, главная тема для обсуждения с огромным отрывом - ковид, кроме него люди обсуждаю обычный быт - цены, еду и магазины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKSGRyI-uor0"
   },
   "source": [
    "Также выведите 20 самых непопулярных слов (если самых непопулярных слов больше, выведите любые 20 из них) Почему эти токены непопулярны, требуется ли как-то дополнительно работать с ними?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "moArbwfvun9t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466                        1\n",
       "pmt                        1\n",
       "acres                      1\n",
       "cucumber                   1\n",
       "https://t.co/x69jqbisox    1\n",
       "had!                       1\n",
       "massive.                   1\n",
       "https://t.co/6ytxpz5ug9    1\n",
       "https://t.co/l8jnzxjgwo    1\n",
       "readiness,                 1\n",
       "kick-in                    1\n",
       "(covid-19,                 1\n",
       "impacts)                   1\n",
       "https://t.co/wa7kcdwqea    1\n",
       "@linkedin                  1\n",
       "clare                      1\n",
       "connors                    1\n",
       "levins                     1\n",
       "587-4272                   1\n",
       "https://t.co/7j2y3rsld9    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRp3J1gQunlR"
   },
   "source": [
    "**Ответ:** Самые непопулярные токены - это ссылки и необычные формы слов. На этом примере хорошо видно проблему такого токенизатора, он не учитывает формы слов и соседние символы, например, в этом списке есть \"(covid-19,\". Тема ковида - одна из популярнейших, но комбинация соседних символов вывела этот токен вниз. Для улучшения работы модели это нужно исправить\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx9LQOSPzvjV"
   },
   "source": [
    "Теперь воспользуемся токенайзером получше - TweetTokenizer из библиотеки nltk. Примените его и посмотрите на топ-10 популярных слов. Чем он отличается от топа, который получался раньше? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "2G1UkyVxzvFY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the             26993\n",
       ".               24118\n",
       "to              23478\n",
       ",               17571\n",
       "and             14825\n",
       "of              13044\n",
       "a               11891\n",
       "in              11348\n",
       "?                9524\n",
       "#coronavirus     8808\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "dictionary = dict()\n",
    "\n",
    "for tweet in tweets:\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        dictionary[token.lower()] = dictionary.get(token.lower(), 0) + 1\n",
    "\n",
    "dictionary_sorted = pd.Series(dictionary)\n",
    "dictionary_sorted = dictionary_sorted.sort_values(ascending=False)\n",
    "dictionary_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50eVUnJN1Zxl"
   },
   "source": [
    "**Ответ:** В отличие от простого токенизатора, теперь в топ, кроме вспомогательных частей речи попали символы, которые ранее были частью слов. Теперь они стали полноценными токенами, и очень популярными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gqQgiMs11bs"
   },
   "source": [
    "Удалите из словаря стоп-слова и пунктуацию, посмотрите на новый топ-10 слов с количеством встреч, есть ли теперь в нем что-то не похожее на слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "0yHWdFrp0Mup"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#coronavirus    8808\n",
       "â               7415\n",
       "               7311\n",
       "19              7167\n",
       "covid           6253\n",
       "prices          4601\n",
       "               4372\n",
       "food            4367\n",
       "store           3877\n",
       "supermarket     3805\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "new_dictionary = {key: value for key, value in dictionary.items() if (key not in stop_words) and (key not in list(punctuation))}\n",
    "no_stop_words = pd.Series(new_dictionary)\n",
    "no_stop_words = no_stop_words.sort_values(ascending=False)\n",
    "no_stop_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZJqXELP_Yxy"
   },
   "source": [
    "**Ответ:** Теперь топ-10 похож на тот,который получали ранее с обычным токенизатором: ковид и бытовые тема, но также несколько странных и неотображаемых символов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzXjMsSB_kXB"
   },
   "source": [
    "Скорее всего в некоторых топах были неотображаемые символы или отдельные буквы не латинского алфавита. Уберем их: удалите из словаря токены из одного символа, позиция которого в таблице Unicode 128 и более (`ord(x) >= 128`)\n",
    "\n",
    "Выведите топ-10 самых популярных и топ-20 непопулярных слов. Чем полученные топы отличаются от итоговых топов, полученных при использовании токенизации по пробелам? Что теперь лучше, а что хуже?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "1695hlkS_1-J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#coronavirus    8808\n",
       "19              7167\n",
       "covid           6253\n",
       "prices          4601\n",
       "food            4367\n",
       "store           3877\n",
       "supermarket     3805\n",
       "grocery         3523\n",
       "people          3463\n",
       "#covid19        2589\n",
       "dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filt(x):\n",
    "    if len(x) <= 1:\n",
    "        if ord(x) >= 128:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "new_new_dictionary = {key: value for key, value in new_dictionary.items() if filt(key)}\n",
    "final_dict = pd.Series(new_new_dictionary)\n",
    "final_dict = final_dict.sort_values(ascending=False)\n",
    "final_dict.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@awgcorporate              1\n",
       "https://t.co/l8jnzxjgwo    1\n",
       "587-4272                   1\n",
       "readiness                  1\n",
       "kick-in                    1\n",
       "https://t.co/wa7kcdwqea    1\n",
       "@linkedin                  1\n",
       "clare                      1\n",
       "connors                    1\n",
       "levins                     1\n",
       "https://t.co/6ytxpz5ug9    1\n",
       "https://t.co/8xikga3rel    1\n",
       "quantitatively             1\n",
       "logarithmic                1\n",
       "non-log                    1\n",
       "improveã                   1\n",
       "https://t.co/aacxqg5sej    1\n",
       "rebooked                   1\n",
       "can't-miss                 1\n",
       "https://t.co/7j2y3rsld9    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzjHAKIlDvc6"
   },
   "source": [
    "**Ответ:** топ остался практически таким же, только \"19\" стал отдельным символом. А вот низ рейтинга стал гораздо полезнее для наших целей, теперь там нет необычных комбинаций из слов и знаков пунктуации, теперь тут ссылки и редко используемые слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcDf9_6HB2zm"
   },
   "source": [
    "Выведите топ-10 популярных хештегов (токены, первые символы которых - #) с количеством встреч. Что можно сказать о них?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "zk4fygCUBw3l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#coronavirus            8808\n",
       "#covid19                2589\n",
       "#covid_19               1734\n",
       "#covid2019               946\n",
       "#toiletpaper             744\n",
       "#covid                   641\n",
       "#socialdistancing        465\n",
       "#coronacrisis            448\n",
       "#pandemic                257\n",
       "#coronaviruspandemic     249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag = {key: value for key, value in new_new_dictionary.items() if key[0] == \"#\"}\n",
    "hashtags = pd.Series(hashtag)\n",
    "hashtags = hashtags.sort_values(ascending=False)\n",
    "hashtags.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6NeNWBkDxM7"
   },
   "source": [
    "**Ответ:** Выводы остаются примерно теми же, ковид и бытовые темы, а также призывы к действию (socialdistancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLYBg7caD5GA"
   },
   "source": [
    "То же самое проделайте для ссылок на сайт https://t.co Сравнима ли популярность ссылок с популярностью хештегов? Будет ли информация о ссылке на конкретную страницу полезна?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "MXbm1oeaCK9S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "https://t.co/oxa7swtond    5\n",
       "https://t.co/gp3eusapl8    4\n",
       "https://t.co/kuwipf1kqw    3\n",
       "https://t.co/bylqxrjmnt    3\n",
       "https://t.co/wuieefsnoj    3\n",
       "https://t.co/zjnrx6dkkn    3\n",
       "https://t.co/wrlhyzizaa    3\n",
       "https://t.co/g63rp042ho    3\n",
       "https://t.co/e2znxajpre    3\n",
       "https://t.co/catkegayoy    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = {key: value for key, value in new_new_dictionary.items() if key.startswith(\"https://t.co\")}\n",
    "links = pd.Series(link)\n",
    "links = links.sort_values(ascending=False)\n",
    "links.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at6lRYZ8A07N"
   },
   "source": [
    "**Ответ:** Ссылки используются ГОРАЗДО реже хештегов, одна ссылка участвует в 5 постах, что не позволяет использовать их для поиска зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOGdUU1kBU1D"
   },
   "source": [
    "Используем опыт предыдущих экспериментов и напишем собственный токенайзер, улучшив TweetTokenizer. Функция tokenize должна:\n",
    "\n",
    "\n",
    "\n",
    "*   Привести текст в нижний регистр\n",
    "*   Применить TweetTokenizer для  выделения токенов\n",
    "*   Удалить стоп-слова, пунктуацию, токены из одного символа с позицией в таблице Unicode 128 и более,  ссылки на t.co\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "ctEsB6xkFrrK"
   },
   "outputs": [],
   "source": [
    "def filt(x):\n",
    "    if len(x) <= 1:\n",
    "        if ord(x) >= 128:\n",
    "            return False\n",
    "    if x.startswith(\"https://t.co\"):\n",
    "        return False\n",
    "    if x in stop_words:\n",
    "        return False\n",
    "    if x in list(punctuation):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "\n",
    "  tokens = tweet_tokenizer.tokenize(text.lower())\n",
    "\n",
    "  return [token for token in tokens if filt(token)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwbgtYkJGYym",
    "outputId": "5808765b-3448-45e6-ccc1-7cd65f6371ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'text', '@sample_text', '#sampletext']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer('This is sample text!!!! @Sample_text I, \\x92\\x92 https://t.co/sample  #sampletext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wURVABmXHk97"
   },
   "source": [
    "## Задание 3 Векторизация текстов (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H44iXkoHIQfN"
   },
   "source": [
    "Обучите CountVectorizer с использованием custom_tokenizer в качестве токенайзера. Как размер полученного словаря соотносится с размером изначального словаря из начала задания 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHn_limQl3BI",
    "outputId": "8e9c1826-319f-4376-f06e-c30c2eb82648"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prusakovR\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "cv.fit_transform(X_train[\"OriginalTweet\"])\n",
    "\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsfmaSGoItUm"
   },
   "source": [
    "**Ответ:** # было 79755, стал почти в 2 раза меньше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm6UHNmqKZT0"
   },
   "source": [
    "Посмотрим на какой-нибудь конкретный твитт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "aJVjjfqOJh8m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Shop keepers taking advantage of #Coronavirus boosting prices disproportionately so the most Marginalised suffer who can't afford it #SHAMEONYOU #Wewillremember\",\n",
       " 0)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 9023\n",
    "df.iloc[ind]['OriginalTweet'], df.iloc[ind]['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBMIHBI5KdaS"
   },
   "source": [
    "Автор твитта не доволен ситуацией с едой во Франции и текст имеет резко негативную окраску.\n",
    "\n",
    "Примените обученный CountVectorizer для векторизации данного текста, и попытайтесь определить самый важный токен и самый неважный токен (токен, компонента которого в векторе максимальна/минимальна, без учета 0). Хорошо ли они определились, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "7NcAllaEKsJj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#wewillremember       1\n",
       "can't                 1\n",
       "shop                  1\n",
       "afford                1\n",
       "disproportionately    1\n",
       "#coronavirus          1\n",
       "boosting              1\n",
       "prices                1\n",
       "#shameonyou           1\n",
       "suffer                1\n",
       "taking                1\n",
       "marginalised          1\n",
       "keepers               1\n",
       "advantage             1\n",
       "interest              0\n",
       "interactions          0\n",
       "intereste             0\n",
       "interesed             0\n",
       "interested            0\n",
       "interesting           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "france_eat = cv.transform([df.iloc[ind]['OriginalTweet']]).toarray()\n",
    "tokens = pd.Series(france_eat.reshape(-1), index=cv.get_feature_names_out())\n",
    "tokens = tokens.sort_values(ascending=False)\n",
    "tokens.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpEsl1k_NF4T"
   },
   "source": [
    "**Ответ:** Векторизация дала одинаковый вес всем токенам в тексте, что не помогает нам в поиске самого важного и неважного токена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4DsEQpLO3J6"
   },
   "source": [
    "Теперь примените TfidfVectorizer и  определите самый важный/неважный токены. Хорошо ли определились, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "uSNzdK3ENGB3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prusakovR\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "#wewillremember       0.373460\n",
       "marginalised          0.373460\n",
       "#shameonyou           0.348492\n",
       "keepers               0.333887\n",
       "disproportionately    0.328334\n",
       "boosting              0.306035\n",
       "suffer                0.266462\n",
       "afford                0.230787\n",
       "advantage             0.211172\n",
       "can't                 0.196633\n",
       "taking                0.187111\n",
       "shop                  0.172201\n",
       "prices                0.096845\n",
       "#coronavirus          0.071426\n",
       "interactive           0.000000\n",
       "intercept             0.000000\n",
       "intercession          0.000000\n",
       "interconnected        0.000000\n",
       "interests             0.000000\n",
       "interesed             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "tfidf.fit(X_train[\"OriginalTweet\"])\n",
    "\n",
    "france_eat = tfidf.transform([df.iloc[ind]['OriginalTweet']]).toarray()\n",
    "tokens = pd.Series(france_eat.reshape(-1), index=cv.get_feature_names_out())\n",
    "tokens = tokens.sort_values(ascending=False)\n",
    "tokens.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYao_UhqQADm"
   },
   "source": [
    "**Ответ:** Такая векторизация показывает себя гораздо лучше, самый бесполезный токен - \"#coronavirus\", так как он встричается очень часто и не дает никакой подсказки об эмоциональной окраске твита, а вот самые популярные токены показывают сильный негативный оттенок. Вывод - TFIDF токенизация действительно хорошо сказывается на эффективности нашей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGRJPqfWSesQ"
   },
   "source": [
    "Найдите какой-нибудь положительно окрашенный твитт, где TfidfVectorizer хорошо (полезно для определения окраски) выделяет важный токен, поясните пример.\n",
    "\n",
    "*Подсказка:* явно положительные твитты можно искать при помощи положительных слов (good, great, amazing и т. д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "bRbQ2CHiSuJI",
    "outputId": "c4b34a7d-1076-4e1e-ad5c-9466fd2097c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>4091</td>\n",
       "      <td>49043</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Loving how quiet the Tube is atm. COVID-19 is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>5946</td>\n",
       "      <td>50898</td>\n",
       "      <td>West Midlands, UK</td>\n",
       "      <td>17-03-2020</td>\n",
       "      <td>@mcdonalds your free drink for social and emer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>6980</td>\n",
       "      <td>51932</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>18-03-2020</td>\n",
       "      <td>@TheSun The panic buying will continue for wee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>7962</td>\n",
       "      <td>52914</td>\n",
       "      <td>Spalding, England</td>\n",
       "      <td>18-03-2020</td>\n",
       "      <td>Healthcare workers are fantastic! I would also...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>8619</td>\n",
       "      <td>53571</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>18-03-2020</td>\n",
       "      <td>@AmyGorin shares a fantastic list of foods rec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>9517</td>\n",
       "      <td>54469</td>\n",
       "      <td>London, England</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>Thanks to great teamwork from colleagues at @W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>10654</td>\n",
       "      <td>55606</td>\n",
       "      <td>Coventry, ENG ???????</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>@drphilhammond to clarify: our pub is fantasti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>15113</td>\n",
       "      <td>60065</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>20-03-2020</td>\n",
       "      <td>For all the grocery store workers during this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15864</th>\n",
       "      <td>23086</td>\n",
       "      <td>68038</td>\n",
       "      <td>Cumbria &amp; South West Scotland</td>\n",
       "      <td>24-03-2020</td>\n",
       "      <td>NEW DEALS AVAILABLE EVERYDAY When browsing our...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16627</th>\n",
       "      <td>24042</td>\n",
       "      <td>68994</td>\n",
       "      <td>Saskatoon, Canada</td>\n",
       "      <td>24-03-2020</td>\n",
       "      <td>Absolutely fantastic TEDx Talk.\\r\\r\\nWhy are p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16754</th>\n",
       "      <td>24196</td>\n",
       "      <td>69148</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>25-03-2020</td>\n",
       "      <td>Town have pledged to pay tribute to the fant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21809</th>\n",
       "      <td>30421</td>\n",
       "      <td>75373</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>02-04-2020</td>\n",
       "      <td>Take a look at this week s enewsletter to find...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23398</th>\n",
       "      <td>32384</td>\n",
       "      <td>77336</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>04-04-2020</td>\n",
       "      <td>Say goodbye to outrageous monthly TV subscript...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24038</th>\n",
       "      <td>33181</td>\n",
       "      <td>78133</td>\n",
       "      <td>lewis.goodall@bbc.co.uk</td>\n",
       "      <td>05-04-2020</td>\n",
       "      <td>Our supermarket staff are doing such a fantast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24489</th>\n",
       "      <td>33756</td>\n",
       "      <td>78708</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>06-04-2020</td>\n",
       "      <td>Thanks @AldiUSA for the fantastic online shopp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserName  ScreenName                       Location     TweetAt  \\\n",
       "242        4091       49043                             UK  16-03-2020   \n",
       "1760       5946       50898              West Midlands, UK  17-03-2020   \n",
       "2631       6980       51932                        Unknown  18-03-2020   \n",
       "3429       7962       52914              Spalding, England  18-03-2020   \n",
       "3964       8619       53571                  San Francisco  18-03-2020   \n",
       "4707       9517       54469                London, England  19-03-2020   \n",
       "5645      10654       55606          Coventry, ENG ???????  19-03-2020   \n",
       "9322      15113       60065               Toronto, Ontario  20-03-2020   \n",
       "15864     23086       68038  Cumbria & South West Scotland  24-03-2020   \n",
       "16627     24042       68994              Saskatoon, Canada  24-03-2020   \n",
       "16754     24196       69148                        Unknown  25-03-2020   \n",
       "21809     30421       75373                        Unknown  02-04-2020   \n",
       "23398     32384       77336                        Unknown  04-04-2020   \n",
       "24038     33181       78133        lewis.goodall@bbc.co.uk  05-04-2020   \n",
       "24489     33756       78708                    Chicago, IL  06-04-2020   \n",
       "\n",
       "                                           OriginalTweet  Sentiment  \n",
       "242    Loving how quiet the Tube is atm. COVID-19 is ...          1  \n",
       "1760   @mcdonalds your free drink for social and emer...          1  \n",
       "2631   @TheSun The panic buying will continue for wee...          1  \n",
       "3429   Healthcare workers are fantastic! I would also...          1  \n",
       "3964   @AmyGorin shares a fantastic list of foods rec...          1  \n",
       "4707   Thanks to great teamwork from colleagues at @W...          1  \n",
       "5645   @drphilhammond to clarify: our pub is fantasti...          1  \n",
       "9322   For all the grocery store workers during this ...          1  \n",
       "15864  NEW DEALS AVAILABLE EVERYDAY When browsing our...          1  \n",
       "16627  Absolutely fantastic TEDx Talk.\\r\\r\\nWhy are p...          1  \n",
       "16754    Town have pledged to pay tribute to the fant...          1  \n",
       "21809  Take a look at this week s enewsletter to find...          1  \n",
       "23398  Say goodbye to outrageous monthly TV subscript...          1  \n",
       "24038  Our supermarket staff are doing such a fantast...          1  \n",
       "24489  Thanks @AldiUSA for the fantastic online shopp...          1  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['OriginalTweet'].apply(lambda x: 'fantastic' in x) & (df['Sentiment'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "jSjbKPCWk87K"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "turnover        0.467661\n",
       "@thesun         0.454767\n",
       "fantastic       0.440007\n",
       "#panicbuying    0.297489\n",
       "continue        0.280454\n",
       "weeks           0.256627\n",
       "#covid2019      0.215935\n",
       "buying          0.210145\n",
       "panic           0.189423\n",
       "supermarket     0.145574\n",
       "dtype: float64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = custom_tokenizer(df.iloc[2631]['OriginalTweet'])\n",
    "result = tfidf.transform([df.iloc[2631]['OriginalTweet']]).toarray()\n",
    "\n",
    "result = pd.Series(result.reshape(-1), index=tfidf.get_feature_names_out())\n",
    "result = result.reindex(tweets)\n",
    "result = result.sort_values(ascending=False)\n",
    "result.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTv9ST2_U6NA"
   },
   "source": [
    "**Ответ:** Взял 2631-ый твит. Я выбрал слово fantastic, как слово с явно позитивной окраской, проиндксировав выбранный твит, мое слово и правда стало одним из 3-ех самых важных токенов, однако токены turnover и @thesun важнее, чем fantastic. Я полагаю, что это связано с распространенностью данного токена, поэтому более конкретные слова оказывают больше влияния на вывод векторизатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVEuZm8BHms6"
   },
   "source": [
    "## Задание 4 Обучение первых моделей (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JADkO3sfXdOG"
   },
   "source": [
    "Примените оба векторайзера для получения матриц с признаками текстов.  Выделите целевую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "DguoiXhCX2oN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m cv_test \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginalTweet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m tfidf_train \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginalTweet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m tfidf_test \u001b[38;5;241m=\u001b[39m tfidf_vec\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginalTweet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vec' is not defined"
     ]
    }
   ],
   "source": [
    "cv_train = cv.transform(X_train['OriginalTweet'])\n",
    "cv_test = cv.transform(X_test['OriginalTweet'])\n",
    "\n",
    "tfidf_train = tfidf.transform(X_train['OriginalTweet'])\n",
    "tfidf_test = tfidf_vec.transform(X_test['OriginalTweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FX1KSOfYSx4"
   },
   "source": [
    "Обучите логистическую регрессию на векторах из обоих векторайзеров. Посчитайте долю правильных ответов на обучающих и тестовых данных. Какой векторайзер показал лучший результат? Что можно сказать о моделях?\n",
    "\n",
    "Используйте `sparse` матрицы (после векторизации), не превращайте их в `numpy.ndarray` или `pd.DataFrame` - может не хватить памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Tb3eh8UXJ6v"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cv_regression = LogisticRegression()\n",
    "tfidf_regression = LogisticRegression()\n",
    "\n",
    "cv_regression.fit(cv_train, y_train)\n",
    "tfidf_regression.fit(tfidf_train, y_train)\n",
    "\n",
    "cv_train_pred = cv_regression.predict(cv_train)\n",
    "tfidf_train_pred = tfidf_regression.predict(tfidf_train)\n",
    "\n",
    "cv_test_pred = cv_regression.predict(cv_test)\n",
    "tfidf_test_pred = tfidf_regression.predict(tfidf_test)\n",
    "\n",
    "print(\"cv_train_pred\", accuracy_score(cv_train_pred, y_train))\n",
    "print(\"cv_test_pred\", accuracy_score(cv_test_pred, y_test))\n",
    "print()\n",
    "print(\"tfidf_train_pred\", accuracy_score(tfidf_train_pred, y_train))\n",
    "print(\"tfidf_test_pred\", accuracy_score(tfidf_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y_wO7rCmv7K"
   },
   "source": [
    "**Ответ:** # На нашем датасете лучше себя показывает более простая модель регрессии, полагаю, что это связано с простотой задачи, tfidf просто не успевает раскрыться "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSOR1i3mjrys"
   },
   "source": [
    "## Задание 5 Стемминг (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6ONBWNPjuq-"
   },
   "source": [
    "Для уменьшения словаря можно использовать стемминг.\n",
    "\n",
    "Модифицируйте написанный токенайзер, добавив в него стемминг с использованием SnowballStemmer. Обучите Count- и Tfidf- векторайзеры. Как изменился размер словаря?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "oVfA2-iMkQBb"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def custom_stem_tokenizer(text):\n",
    "    tokens = []\n",
    "    stem = [stemmer.stem(t) for t in tokenizer.tokenize(text)]\n",
    "    \n",
    "    for x in stem:\n",
    "        if len(x) <= 1:\n",
    "            if ord(x) >= 128:\n",
    "                continue\n",
    "        if x.startswith(\"https://t.co\"):\n",
    "            continue\n",
    "        if x in stop_words:\n",
    "            continue\n",
    "        if x in list(punctuation):\n",
    "            continue\n",
    "\n",
    "        tokens.append(x)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QmrjYtqnlPd",
    "outputId": "cd91291d-9676-4611-9fc4-28afaed58963"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sampl', 'text', '@sample_text', '#sampletext', 'ad', 'word', 'check', 'stem']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stem_tokenizer('This is sample text!!!! @Sample_text I, \\x92\\x92 https://t.co/sample  #sampletext adding more words to check stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAvUTmaplzOS",
    "outputId": "566207fe-183b-4ed6-d333-f86f0cc9ae38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36608\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=custom_stem_tokenizer)\n",
    "cv.fit(X_train[\"OriginalTweet\"])\n",
    "\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyzs5TaAoHP6"
   },
   "source": [
    "**Ответ** # Словарь стал меньше, так как уменьшилось кол-во форм одних и тех же слов, так как при стемминге удаляется аффикс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OkncHI8oRmd"
   },
   "source": [
    "Обучите логистическую регрессию с использованием обоих векторайзеров. Изменилось ли качество? Есть ли смысл применять стемминг?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "ykZJPphEoZ5W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prusakovR\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_train_pred 0.9717214865442119\n",
      "cv_test_pred 0.8681482957942994\n",
      "\n",
      "tfidf_train_pred 0.9155489107219137\n",
      "tfidf_test_pred 0.8546940402631055\n"
     ]
    }
   ],
   "source": [
    "count_vc_stemed = CountVectorizer(tokenizer=custom_stem_tokenizer)\n",
    "tfidf_vc_stemed = TfidfVectorizer(tokenizer=custom_stem_tokenizer)\n",
    "\n",
    "count_vc_stemed.fit(X_train[\"OriginalTweet\"])\n",
    "tfidf_vc_stemed.fit(X_train[\"OriginalTweet\"])\n",
    "\n",
    "cv_test = count_vc_stemed.transform(X_test[\"OriginalTweet\"])\n",
    "tfidf_test = tfidf_vc_stemed.transform(X_test[\"OriginalTweet\"])\n",
    "\n",
    "cv_train = count_vc_stemed.transform(X_train[\"OriginalTweet\"])\n",
    "tfidf_train = tfidf_vc_stemed.transform(X_train[\"OriginalTweet\"])\n",
    "\n",
    "cv_regression = LogisticRegression()\n",
    "tfidf_regression = LogisticRegression()\n",
    "\n",
    "cv_regression.fit(cv_train, y_train)\n",
    "tfidf_regression.fit(tfidf_train, y_train)\n",
    "\n",
    "cv_train_pred = cv_regression.predict(cv_train)\n",
    "tfidf_train_pred = tfidf_regression.predict(tfidf_train)\n",
    "\n",
    "cv_test_pred = cv_regression.predict(cv_test)\n",
    "tfidf_test_pred = tfidf_regression.predict(tfidf_test)\n",
    "\n",
    "print(\"cv_train_pred\", accuracy_score(cv_train_pred, y_train))\n",
    "print(\"cv_test_pred\", accuracy_score(cv_test_pred, y_test))\n",
    "print()\n",
    "print(\"tfidf_train_pred\", accuracy_score(tfidf_train_pred, y_train))\n",
    "print(\"tfidf_test_pred\", accuracy_score(tfidf_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCRlrODro0h8"
   },
   "source": [
    "**Ответ:** В данном примере использовать стемминг нет смысла, так как прироста к качеству работы модели нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYWGQNEDqLC-"
   },
   "source": [
    "## Задание  6 Работа с частотами (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hq-tl5mqUSn"
   },
   "source": [
    "Еще один способ уменьшить количество признаков - это использовать параметры min_df и max_df при построении векторайзера  эти параметры помогают ограничить требуемую частоту встречаемости токена в документах.\n",
    "\n",
    "По умолчанию берутся все токены, которые встретились хотя бы один раз.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1SiD4DE3WZ2"
   },
   "source": [
    "Подберите max_df такой, что размер словаря будет 36651 (на 1 меньше, чем было). Почему параметр получился такой большой/маленький?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3YLb8PViExb",
    "outputId": "b6d67654-d232-4e11-a5ca-6f2145053e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36651\n"
     ]
    }
   ],
   "source": [
    "cv_df = CountVectorizer(tokenizer=custom_stem_tokenizer,\n",
    "                        max_df=# -- YOUR CODE HERE --\n",
    "                        ).fit(\n",
    "                            # -- YOUR CODE HERE --\n",
    "                            )\n",
    "print(len(cv_df.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyEpkJUkjnuK"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdZYoGZR4UsA"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gRIUaB1u32f"
   },
   "source": [
    "Подберите min_df (используйте дефолтное значение max_df) в CountVectorizer таким образом, чтобы размер словаря был 3700 токенов (при использовании токенайзера со стеммингом), а качество осталось таким же, как и было. Что можно сказать о результатах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSnMJkn9XmsT",
    "outputId": "e0d8eb21-e5d7-46b4-e1d1-4b1ae220e9a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n"
     ]
    }
   ],
   "source": [
    "cv_df = CountVectorizer(tokenizer=custom_stem_tokenizer,\n",
    "                        min_df=# -- YOUR CODE HERE --\n",
    "                        ).fit(\n",
    "                            # -- YOUR CODE HERE --\n",
    "                            )\n",
    "print(len(cv_df.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvMDwpdfjm8Y"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fGYpUIZx0fk"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx_h_-inKbBl"
   },
   "source": [
    "В предыдущих заданиях признаки не скалировались. Отскалируйте данные (при словаре размера 3.7 тысяч, векторизованные CountVectorizer), обучите логистическую регрессию, посмотрите качество и выведите `barplot`, содержащий по 10 токенов, с наибольшим по модулю положительными/отрицательными весами. Что можно сказать об этих токенах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBATXJX6LG9q"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThcEfzY1LHET"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktJVOdrIHq7B"
   },
   "source": [
    "## Задание 7 Другие признаки (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt3jRCZ2H0Og"
   },
   "source": [
    "Мы были сконцентрированы на работе с текстами твиттов и не использовали другие признаки - имена пользователя, дату и местоположение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52wjewCCo_di"
   },
   "source": [
    "Изучите признаки UserName и ScreenName. полезны ли они? Если полезны, то закодируйте их, добавьте к матрице с отскалированными признаками, обучите логистическую регрессию, замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63thouYZptj6"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8_qR-gnpT3a"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ythEcFSkt7y3"
   },
   "source": [
    "Изучите признак TweetAt в обучающей выборке: преобразуйте его к типу datetime и нарисуйте его гистограмму с разделением по цвету на основе целевой переменной. Полезен ли он? Если полезен, то закодируйте его, добавьте к матрице с отскалированными признаками, обучите логистическую регрессию, замерьте качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lxb_k0JLirNv"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IdLBdpQxM-G"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2JtRPhNP6qx"
   },
   "source": [
    "Поработайте с признаком Location в обучающей выборке. Сколько уникальных значений?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYQZQ1FRNpoe"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k4JwpRTQISa"
   },
   "source": [
    "Постройте гистограмму топ-10 по популярности местоположений (исключая Unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J91YkhegJ0mz"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOsv3lODTfYB"
   },
   "source": [
    "Видно, что многие местоположения включают в себя более точное название места, чем другие (Например, у некоторых стоит London, UK; а у некоторых просто UK или United Kingdom).\n",
    "\n",
    "Создайте новый признак WiderLocation, который содержит самое широкое местоположение (например, из London, UK должно получиться UK). Сколько уникальных категорий теперь? Постройте аналогичную гистограмму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSkow6acOMyD"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgyWrD2eVfff"
   },
   "source": [
    "Закодируйте признак WiderLocation с помощью OHE таким образом, чтобы создались только столбцы для местоположений, которые встречаются более одного раза. Сколько таких значений?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeJBfBWgPvg_"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyMX5kZuimPK"
   },
   "source": [
    "Добавьте этот признак к матрице отскалированных текстовых признаков, обучите логистическую регрессию, замерьте качество. Как оно изменилось? Оказался ли признак полезным?\n",
    "\n",
    "\n",
    "*Подсказка:* используйте параметр `categories` в энкодере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EO1jNPeeim7A"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dHsGlDRYUQt"
   },
   "source": [
    "**Ответ:** # -- YOUR ANSWER HERE --"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
